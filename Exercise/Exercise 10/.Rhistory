lines(x = c(i, i), y = c(lower, upper))
}
K.max = 40 # Maximum number of neighbours
### Container to store CV misclassification rates
mis.CV = rep(0, times = K.max)
for(i in 1:K.max){
### Progress update
print(paste0(i, " of ", K.max))
### Fit leave-one-out CV
this.knn = knn(X.train, X.valid, cl = set1[,19], k=i)
### Get and store CV misclassification rate
this.mis.CV = mean(this.knn != Y.train)
mis.CV[i] = this.mis.CV
}
### Get SEs
SE.mis.CV = sapply(mis.CV, function(r){
sqrt(r*(1-r)/nrow(X.train))
})
plot(1:K.max, mis.CV, xlab = "number of neighbours", ylab = "Misclassification Rate",
ylim = c(0.4, 0.64))
mis.CV
plot(1:K.max, mis.CV, xlab = "number of neighbours", ylab = "Misclassification Rate",
ylim = c(0.0, 1))
for(i in 1:K.max){
lower = mis.CV[i] - SE.mis.CV[i]
upper = mis.CV[i] + SE.mis.CV[i]
lines(x = c(i, i), y = c(lower, upper))
}
plot(1:K.max, mis.CV, xlab = "number of neighbours", ylab = "Misclassification Rate",
ylim = c(0.6, 1))
for(i in 1:K.max){
lower = mis.CV[i] - SE.mis.CV[i]
upper = mis.CV[i] + SE.mis.CV[i]
lines(x = c(i, i), y = c(lower, upper))
}
### Get CV min value for K
k.min = which.min(mis.CV)
thresh = mis.CV[k.min] + SE.mis.CV[k.min]
abline(h = thresh, col = "red")
}
K.min
k.min
### Get CV 1SE value for K
k.1se = max(which(mis.CV <= thresh))
k.1se
### Finally, let's see how our tuned KNN models do
knn.min = knn(X.train, X.valid, Y.train, k.min)
knn.1se = knn(X.train, X.valid, Y.train, k.1se)
table(knn.min, Y.valid, dnn = c("Predicted", "Observed"))
table(knn.1se, Y.valid, dnn = c("Predicted", "Observed"))
(mis.min = mean(Y.valid != knn.min))
(mis.1se = mean(Y.valid != knn.1se))
SE.mis.CV
### Finally, let's see how our tuned KNN models do
knn.min = knn(X.train, X.valid, Y.train, k.min)
knn.min
### Get CV min value for K
k.min = which.min(mis.CV)
### Get CV 1SE value for K
k.1se = max(which(mis.CV <= thresh))
### Finally, let's see how our tuned KNN models do
knn.min = knn(X.train, X.valid, Y.train, k.min)
knn.1se = knn(X.train, X.valid, Y.train, k.1se)
(mis.min = mean(Y.valid != knn.min))
(mis.1se = mean(Y.valid != knn.1se))
library(dplyr)
library(MASS)   # For ridge regression
library(glmnet) # For LASSO
library(nnet) # Fits neural net models
library(rgl)
library(FNN)
source("Helper Functions.R")
setwd("C:/Users/injoo/OneDrive/Desktop/SFU/STAT 452/Exercise/Exercise 10")
library(dplyr)
library(MASS)   # For ridge regression
library(glmnet) # For LASSO
library(nnet) # Fits neural net models
library(rgl)
library(FNN)
source("Helper Functions.R")
### Some of our code will be random, so we have to set the seed.
### Use Mersenne-Twister for compatibility.
set.seed(46685326, kind = "Mersenne-Twister")
###### 1. Get to know the data
# (a) Read the data and print a summary.
vehdata = read.csv("vehicle.csv")
summary(X.train)
library(dplyr)
library(MASS)   # For ridge regression
library(glmnet) # For LASSO
library(nnet) # Fits neural net models
library(rgl)
library(FNN)
source("Helper Functions.R")
###### 1. Run logistic regression using multinom().
vehdata = read.csv("vehicle.csv")
vehdata$class = factor(vehdata$class, labels=c("2D", "4D", "BUS", "VAN"))
set.seed(46685326, kind = "Mersenne-Twister")
perm <- sample (x= nrow ( vehdata ))
set1 <- vehdata [ which ( perm <= 3* nrow ( vehdata )/4) , ]
set2 <- vehdata [ which ( perm > 3* nrow ( vehdata )/4) , ]
X.train.raw = set1[,-19]
X.valid.raw = set2[,-19]
Y.train = as.matrix(set1[,19])
Y.valid = as.matrix(set2[,19])
### Rescale x1 using the means and SDs of x2
scale.1 <- function(x1,x2){
for(col in 1:ncol(x1)){
a <- mean(x2[,col])
b <- sd(x2[,col])
x1[,col] <- (x1[,col]-a)/b
}
x1
}
### Rescale our training and validation sets
X.train = scale.1(X.train.raw, X.train.raw)
X.valid = scale.1(X.valid.raw, X.train.raw) # Watch the order
summary(X.train)
### Rescale our training and validation sets
X.train = scale.1(X.train.raw, X.train.raw)
X.valid = scale.1(X.valid.raw, X.train.raw) # Watch the order
summary(X.train)
X.train.raw = set1[,-19]
summary(X.train.raw)
### Rescale our training and validation sets
X.train = scale.1(X.train.raw, X.train.raw)
X.valid = scale.1(X.valid.raw, X.train.raw) # Watch the order
summary(X.train)
summary(X.valid)
head(X.train, 3)
head(X.valid, 3)
X.train.raw = set1
X.valid.raw = set2
Y.train = as.matrix(set1[,19])
Y.valid = as.matrix(set2[,19])
### Rescale x1 using the means and SDs of x2
scale.1 <- function(x1,x2){
for(col in 1:ncol(x1)){
a <- mean(x2[,col])
b <- sd(x2[,col])
x1[,col] <- (x1[,col]-a)/b
}
x1
}
### Rescale our training and validation sets
X.train = scale.1(X.train.raw, X.train.raw)
X.train.raw = set1[,-19]
X.valid.raw = set2[,-19]
Y.train = as.matrix(set1[,19])
Y.valid = as.matrix(set2[,19])
### Rescale x1 using the means and SDs of x2
scale.1 <- function(x1,x2){
for(col in 1:ncol(x1)){
a <- mean(x2[,col])
b <- sd(x2[,col])
x1[,col] <- (x1[,col]-a)/b
}
x1
}
### Rescale our training and validation sets
X.train = scale.1(X.train.raw, X.train.raw)
X.valid = scale.1(X.valid.raw, X.train.raw) # Watch the order
summary(X.train)
summary(X.valid)
head(X.train, 3)
head(X.valid, 3)
# (b) Run the logistic regression model using all explanatory variables.
### Fit a logistic regression model using the multinom() function from the
### nnet package.
fit.log.nnet = multinom(class ~ ., data = X.train, maxit = 200)
setwd("C:/Users/injoo/OneDrive/Desktop/SFU/STAT 452/Tutorial/Lecture17_18")
source("Read Wine Data - Class.R")
### Activate packages
library(nnet)   # For logistic regression
library(car)    # For ANOVA after logistic regression with nnet
library(glmnet) # For logistic regression and LASSO
library(MASS)   # For discriminant analysis
### Set random seed, using Mersenne-Twister for compatibility.
set.seed(46536737, kind = "Mersenne-Twister")
### Split the data into training and validation sets
p.train = 0.75
n = nrow(data)
n.train = floor(p.train*n)
ind.random = sample(1:n)
data.train = data[ind.random <= n.train,]
data.valid = data[ind.random > n.train,]
Y.valid = data.valid[,1]
p.train = 0.75
n = nrow(data)
n.train = floor(p.train*n)
ind.random = sample(1:n)
data.train = data[ind.random <= n.train,]
data.valid = data[ind.random > n.train,]
Y.valid = data.valid[,1]
data.train
head(data.train)
### Read-in and process the data
source("Read Wine Data - Class.R")
setwd("C:/Users/injoo/OneDrive/Desktop/SFU/STAT 452/Tutorial/Lecture17_18")
### Read-in and process the data
source("Read Wine Data - Class.R")
### Read-in and process the data
source("Read Wine Data - Class.R")
library(nnet)   # For logistic regression
library(car)    # For ANOVA after logistic regression with nnet
library(glmnet) # For logistic regression and LASSO
library(MASS)   # For discriminant analysis
### Set random seed, using Mersenne-Twister for compatibility.
set.seed(46536737, kind = "Mersenne-Twister")
### Split the data into training and validation sets
p.train = 0.75
n = nrow(data)
n.train = floor(p.train*n)
ind.random = sample(1:n)
data.train = data[ind.random <= n.train,]
data.valid = data[ind.random > n.train,]
Y.valid = data.valid[,1]
head(data.train)
head(data.valid)
setwd("C:/Users/injoo/OneDrive/Desktop/SFU/STAT 452/Exercise/Exercise 10")
library(dplyr)
library(MASS)   # For ridge regression
library(glmnet) # For LASSO
library(nnet) # Fits neural net models
library(rgl)
library(FNN)
source("Helper Functions.R")
###### 1. Run logistic regression using multinom().
vehdata = read.csv("vehicle.csv")
vehdata$class = factor(vehdata$class, labels=c("2D", "4D", "BUS", "VAN"))
set.seed(46685326, kind = "Mersenne-Twister")
perm <- sample (x= nrow ( vehdata ))
set1 <- vehdata [ which ( perm <= 3* nrow ( vehdata )/4) , ]
set2 <- vehdata [ which ( perm > 3* nrow ( vehdata )/4) , ]
data.train = set1
data.valid = set2
Y.valid = data.valid[, 19]
### Rescale x1 using the means and SDs of x2
rescale <- function(x1,x2){
for(col in 1:ncol(x1)){
a <- min(x2[,col])
b <- max(x2[,col])
x1[,col] <- (x1[,col]-a)/(b-a)
}
x1
}
data.train.scale = data.train
data.valid.scale = data.valid
data.train.scale[,-1] = rescale(data.train.scale[,-9], data.train[,-19])
data.valid.scale[,-1] = rescale(data.valid.scale[,-19], data.train[,-19])
### Rescale our training and validation sets
data.train.scale = data.train
data.valid.scale = data.valid
data.train.scale[,-1] = rescale(data.train.scale[,-19], data.train[,-19])
data.valid.scale[,-1] = rescale(data.valid.scale[,-19], data.train[,-19])
summary(X.train)
summary(data.train.scale)
data.train.scale[,-1] = rescale(data.train.scale[,-19], data.train[,-19])
data.valid.scale[,-1] = rescale(data.valid.scale[,-19], data.train[,-19])
summary(data.train.scale)
summary(data.valid.scale)
library(dplyr)
library(MASS)   # For ridge regression
library(glmnet) # For LASSO
library(nnet) # Fits neural net models
library(rgl)
library(FNN)
source("Helper Functions.R")
###### 1. Run logistic regression using multinom().
vehdata = read.csv("vehicle.csv")
vehdata$class = factor(vehdata$class, labels=c("2D", "4D", "BUS", "VAN"))
set.seed(46685326, kind = "Mersenne-Twister")
perm <- sample (x= nrow ( vehdata ))
set1 <- vehdata [ which ( perm <= 3* nrow ( vehdata )/4) , ]
set2 <- vehdata [ which ( perm > 3* nrow ( vehdata )/4) , ]
data.train = set1
data.valid = set2
Y.valid = data.valid[, 19]
### Rescale x1 using the means and SDs of x2
rescale <- function(x1,x2){
for(col in 1:ncol(x1)){
a <- min(x2[,col])
b <- max(x2[,col])
x1[,col] <- (x1[,col]-a)/(b-a)
}
x1
}
### Rescale our training and validation sets
data.train.scale = data.train
data.valid.scale = data.valid
data.train.scale[,-1] = rescale(data.train.scale[,-19], data.train[,-19])
data.valid.scale[,-1] = rescale(data.valid.scale[,-19], data.train[,-19])
summary(data.train.scale)
data.train.scale[,-19] = rescale(data.train.scale[,-19], data.train[,-19])
data.valid.scale[,-19] = rescale(data.valid.scale[,-19], data.train[,-19])
summary(data.train.scale)
vehdata = read.csv("vehicle.csv")
vehdata$class = factor(vehdata$class, labels=c("2D", "4D", "BUS", "VAN"))
set.seed(46685326, kind = "Mersenne-Twister")
perm <- sample (x= nrow ( vehdata ))
set1 <- vehdata [ which ( perm <= 3* nrow ( vehdata )/4) , ]
set2 <- vehdata [ which ( perm > 3* nrow ( vehdata )/4) , ]
data.train = set1
data.valid = set2
Y.valid = data.valid[, 19]
### Rescale x1 using the means and SDs of x2
rescale <- function(x1,x2){
for(col in 1:ncol(x1)){
a <- min(x2[,col])
b <- max(x2[,col])
x1[,col] <- (x1[,col]-a)/(b-a)
}
x1
}
### Rescale our training and validation sets
data.train.scale = data.train
data.valid.scale = data.valid
data.train.scale[,-19] = rescale(data.train.scale[,-19], data.train[,-19])
data.valid.scale[,-19] = rescale(data.valid.scale[,-19], data.train[,-19])
summary(data.train.scale)
summary(data.valid.scale)
head(data.train.scale, 3)
head(data.valid.scale, 3)
# (b) Run the logistic regression model using all explanatory variables.
### Fit a logistic regression model using the multinom() function from the
### nnet package.
fit.log.nnet = multinom(class ~ ., data = data.train.scale, maxit = 200)
summary(fit.log.nnet)
Anova(fit.log.nnet)
pred.class.1 <- predict(mod.fit, newdata=data.train.scale,
type="class")
pred.class.1 <- predict(fit.log.nnet, newdata=data.train.scale,
type="class")
pred.class.2 <- predict(fit.log.nnet, newdata=data.valid.scale,
type="class")
(mul.misclass.train <- mean(ifelse(pred.class.1 == set1$class,
yes=0, no=1)))
(mul.misclass.test <- mean(ifelse(pred.class.2 == set2$class,
yes=0, no=1)))
### Next, let's investigate the LR's performance on the test set
pred.log.nnet = predict(fit.log.nnet, data.valid.scale)
table(Y.valid, pred.log.nnet,        ### Confusion matrix
dnn = c("Observed", "Predicted"))
(misclass.log.nnet = mean(pred.log.nnet != Y.valid)) ### Misclass rate
X.train.scale = as.matrix(data.train.scale[,-1])
Y.train = data.train.scale[,1]
X.valid.scale = as.matrix(data.valid.scale[,-1])
Y.valid = data.valid.scale[,1]
### Let's repeat our logistic regression analysis using the glmnet package.
fit.log.glmnet = glmnet(X.train.scale, Y.train, family = "multinomial")
head(Y.train)
X.train.scale
### Let's repeat our logistic regression analysis using the glmnet package.
fit.log.glmnet = glmnet(X.train.scale, Y.train, family = "multinomial")
# (a) Report a list of the variables included/excluded in each logit. Does
# the pattern seem somewhat consistent with the ANOVA results from
# earlier? Explain in a sentence.
X.train.scale = as.matrix(data.train.scale[,-19])
Y.train = data.train.scale[,19]
X.valid.scale = as.matrix(data.valid.scale[,-19])
Y.valid = data.valid.scale[,19]
### Let's repeat our logistic regression analysis using the glmnet package.
fit.log.glmnet = glmnet(X.train.scale, Y.train, family = "multinomial")
### Get predictions and investigate performance. The predict() function
### for glmnet() can give several different types of output. To get
### predicted values, set type="class". Remember to set s=0 for logistic
### regression.
pred.log.glmnet = predict(fit.log.glmnet, X.valid.scale, type = "class",
s = 0)
table(Y.valid, pred.log.glmnet, dnn = c("Observed", "Predicted"))
(misclass.log.glmnet = mean(Y.valid != pred.log.glmnet))
### Let's repeat our logistic regression analysis using the glmnet package.
fit.log.glmnet = cv.glmnet(X.train.scale, Y.train, family = "multinomial")
### Let's repeat our logistic regression analysis using the glmnet package.
#fit.log.glmnet = cv.glmnet(X.train.scale, Y.train, family = "multinomial")
all.LASSOs = cv.glmnet(X.train.scale, Y.train, family = "multinomial")
lambda.min = all.LASSOs$lambda.min
lambda.1se = all.LASSOs$lambda.1se
lambda.min
lambda.1se
### Get predictions and investigate performance. The predict() function
### for glmnet() can give several different types of output. To get
### predicted values, set type="class". Remember to set s=0 for logistic
### regression.
# pred.log.glmnet = predict(fit.log.glmnet, X.valid.scale, type = "class",
#                           s = 0)
pred.log.glmnet.min = predict(fit.log.glmnet, X.valid.scale, type = "class",
s = lambda.min)
pred.log.glmnet.1se = predict(fit.log.glmnet, X.valid.scale, type = "class",
s = lambda.1se)
# table(Y.valid, pred.log.glmnet, dnn = c("Observed", "Predicted"))
table(Y.valid, pred.log.glmnet.min, dnn = c("Observed", "Predicted"))
table(Y.valid, pred.log.glmnet.1se, dnn = c("Observed", "Predicted"))
(misclass.log.glmnet = mean(Y.valid != pred.log.glmnet.min))
(misclass.log.glmnet = mean(Y.valid != pred.log.glmnet.1se))
preg.log.glmnet.min
pred.log.glmnet.min
fit.log.glmnet
all.LASSOs
# table(Y.valid, pred.log.glmnet, dnn = c("Observed", "Predicted"))
table(Y.valid, pred.log.glmnet.min, dnn = c("Observed", "Predicted"))
summary(pred.log.glmnet.min)
summary(all.LASSOs)
summary(fit.log.glmnet)
summary(pred.log.glmnet.min)
coef(pred.log.glmnet.1se, s = lambda.min)
coef(pred.log.glmnet.1se, s = "lambda.min")
c<-coef(pred.log.glmnet.min,s='lambda.min',exact=TRUE)
c<-coef(pred.log.glmnet.min, s=lambda.min,exact=TRUE)
c<-coef(all.LASSOs, s=lambda.min,exact=TRUE)
inds<-which(c!=0)
variables<-row.names(c)[inds]
inds<-which(c!=0)
c<-coef(all.LASSOs, s="lambda.min",exact=TRUE, family="multinomial")
inds<-which(c!=0)
coef(all.LASSOs, s="lambda.min",exact=TRUE, family="multinomial")
lasso.coef = coef(all.LASSOs, s="lambda.min",exact=TRUE, family="multinomial")
lasso.coef
lasso.min.coef = coef(all.LASSOs, s="lambda.min",exact=TRUE, family="multinomial")
lasso.min.coef = coef(all.LASSOs, s="lambda.min",exact=TRUE, family="multinomial")
lasso.min.coef
lasso.1se.coef = coef(all.LASSOs, s="lambda.1se",exact=TRUE, family="multinomial")
lasso.1se.coef
(misclass.log.glmnet = mean(Y.valid != pred.log.glmnet.min))
(misclass.log.glmnet = mean(Y.valid != pred.log.glmnet.1se))
fit.CV.lasso = cv.glmnet(X.train.scale, Y.train, family = "multinomial")
lambda.min = fit.CV.lasso$lambda.min
lambda.1se = fit.CV.lasso$lambda.1se
coef(fit.CV.lasso, s = lambda.min)
coef(fit.CV.lasso, s = lambda.1se)
pred.lasso.min = predict(fit.CV.lasso, X.valid.scale, s = lambda.min,
type = "class")
pred.lasso.1se = predict(fit.CV.lasso, X.valid.scale, s = lambda.1se,
type = "class")
pred.lasso.min = predict(fit.CV.lasso, X.valid.scale, s = lambda.min,
type = "class")
pred.lasso.1se = predict(fit.CV.lasso, X.valid.scale, s = lambda.1se,
type = "class")
table(Y.valid, pred.lasso.min, dnn = c("Obs", "Pred"))
table(Y.valid, pred.lasso.1se, dnn = c("Obs", "Pred"))
(miss.lasso.min = mean(Y.valid != pred.lasso.min))
(miss.lasso.1se = mean(Y.valid != pred.lasso.1se))
scale.1 <- function(x1,x2){
for(col in 1:ncol(x1)){
a <- mean(x2[,col])
b <- sd(x2[,col])
x1[,col] <- (x1[,col]-a)/b
}
x1
}
X.train.DA = scale.1(data.train[,-1], data.train[,-1])
X.valid.DA = scale.1(data.valid[,-1], data.train[,-1])
##### 3. Run LDA on these data.
# (a) Make a colour plot of the classes against pairs of linear discriminants
scale.1 <- function(x1,x2){
for(col in 1:ncol(x1)){
a <- mean(x2[,col])
b <- sd(x2[,col])
x1[,col] <- (x1[,col]-a)/b
}
x1
}
X.train.DA = scale.1(data.train[,-1], data.train[,-1])
X.train.DA = scale.1(set1[,-19], set1[,-19])
X.valid.DA = scale.1(set2[,-1], set1[,-19])
X.valid.DA = scale.1(set2[,-19], set1[,-19])
### Fit an LDA model using the lda() funtion from the MASS package. This
### function uses predictor/response syntax.
fit.lda = lda(X.train.DA, Y.train)
class.col <- ifelse(set1$class==1,y=53,n= ifelse(set1$class==2,y=68,n=
ifelse(set1$class==3,y=203,n=464)))
### Fit an LDA model using the lda() funtion from the MASS package. This
### function uses predictor/response syntax.
class.col <- ifelse(set1$class==1,y=53,n= ifelse(set1$class==2,y=68,n=
ifelse(set1$class==3,y=203,n=464)))
class.col
set1
set1 <- vehdata [ which ( perm <= 3* nrow ( vehdata )/4) , ]
set2 <- vehdata [ which ( perm > 3* nrow ( vehdata )/4) , ]
### Fit an LDA model using the lda() funtion from the MASS package. This
### function uses predictor/response syntax.
class.col <- ifelse(set1$class==1,y=53,n= ifelse(set1$class==2,y=68,n=
ifelse(set1$class==3,y=203,n=464)))
class.co
class.col
### Fit an LDA model using the lda() funtion from the MASS package. This
### function uses predictor/response syntax.
class.col <- ifelse(set1$class==1,y=53,n= ifelse(set1$class==2,y=68,n=
ifelse(set1$class==3,y=203,n=464)))
class.co
class.col
set1
###### 1. Run logistic regression using multinom().
vehdata = read.csv("vehicle.csv")
set1 <- vehdata [ which ( perm <= 3* nrow ( vehdata )/4) , ]
set2 <- vehdata [ which ( perm > 3* nrow ( vehdata )/4) , ]
### Fit an LDA model using the lda() funtion from the MASS package. This
### function uses predictor/response syntax.
class.col <- ifelse(set1$class==1,y=53,n= ifelse(set1$class==2,y=68,n=
ifelse(set1$class==3,y=203,n=464)))
class.co
class.col
fit.lda = lda(X.train.DA, Y.train)
plot(fit.lda, col = class.col)
X.train.DA
pred.lda = predict(fit.lda, X.valid.DA)$class
table(Y.valid, pred.lda, dnn = c("Obs", "Pred"))
(miss.lda = mean(Y.valid != pred.lda))
fit.qda = qda(X.train.DA, Y.train)
pred.qda = predict(fit.qda, X.valid.DA)$class
table(Y.valid, pred.qda, dnn = c("Obs", "Pred"))
(miss.qda = mean(Y.valid != pred.qda))
