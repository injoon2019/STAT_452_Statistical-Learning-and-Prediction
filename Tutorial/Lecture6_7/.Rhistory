#######################################################################
### Next, let's do ridge regression. This model is fit using the    ###
### lm.ridge() function in the MASS package. We will need to make a ###
### list of candidate lambda values for the function to choose      ###
### from. Prediction also has some extra steps, but we'll discuss   ###
### that when we get there.                                         ###
#######################################################################
### Make a list of lambda values. The lm.ridge() function will
### then choose the best value from this list. Use the seq()
### function to create an equally-spaced list.
lambda.vals = seq(from = 0, to = 100, by = 0.05)
### Use the lm.ridge() function to fit a ridge regression model. The
### syntax is almost identical to the lm() function, we just need
### to set lambda equal to our list of candidate values.
fit.ridge = lm.ridge(alcohol ~ ., lambda = lambda.vals,
data = data.train)
### To get predictions, we need to evaluate the fitted regression
### equation directly (sadly, no predict() function to do this for us).
### You could do this using a for loop if you prefer, but there is
### a shortcut which uses matrix-vector multiplication. The syntax
### for this multiplication method is much shorter.
### Get best lambda value and its index
### Note: Best is chosen according to smallest GCV value. We can
###       get GCV from a ridge regression object using $GCV
ind.min.GCV = which.min(fit.ridge$GCV)
lambda.min = lambda.vals[ind.min.GCV]
### Get coefficients corresponding to best lambda value
### We can get the coefficients for every value of lambda using
### the coef() function on a ridge regression object
all.coefs.ridge = coef(fit.ridge)
coef.min = all.coefs.ridge[ind.min.GCV,]
### We will multiply the dataset by this coefficients vector, but
### we need to add a column to our dataset for the intercept and
### create indicators for our categorical predictors. A simple
### way to do this is using the model.matrix() function from last
### week.
matrix.valid.ridge = model.matrix(alcohol ~ ., data = data.valid)
### Now we can multiply the data by our coefficient vector. The
### syntax in R for matrix-vector multiplication is %*%. Note that,
### for this type of multiplication, order matters. That is,
### A %*% B != B %*% A. Make sure you do data %*% coefficients.
### For more information, see me in a Q&A session or, better still,
### take a course on linear algebra (it's really neat stuff)
pred.ridge = matrix.valid.ridge %*% coef.min
### Now we just need to calculate the MSPE and store it
MSPE.ridge = get.MSPE(Y.valid, pred.ridge)
all.MSPEs[i, "Ridge"] = MSPE.ridge
#######################################################################
### Now we can do the LASSO. This model is fit using the glmnet()   ###
### or cv.glmnet() functions in the glmnet package. LASSO also has  ###
### a tuning parameter, lambda, which we have to choose.            ###
### Fortunately, the cv.glmnet() function does CV internally, and   ###
### lets us automatically find the 'best' value of lambda.          ###
#######################################################################
### The cv.glmnet() function has different syntax from what we're
### used to. Here, we have to provide a matrix with all of our
### predictors, and a vector of our response. LASSO handles
### the intercept differently, so we want to make sure our data
### matrix does not include an intercept (then let cv.glmnet() add
### an intercept later). Unfortunately, the model.matrix() function
### gets confused if we ask it to construct indicators for our
### categorical predictors without also including an intercept.
### A simple way to fix this is to create the data matrix with an
### intercept, then delete the intercept.
matrix.train.raw = model.matrix(alcohol ~ ., data = data.train)
matrix.train = matrix.train.raw[,-1]
### The cv.glmnet() function creates a list of lambda values, then
### does CV internally to choose the 'best' one. 'Best' can refer to
### either the value of lambda which gives the smallest CV-MSPE
### (called the min rule), or the value of lambda which gives the
### simplest model that gives CV-MSPE close to the minimum (called
### the 1se rule). The cv.glmnet() function gets both of these
### lambda values.
all.LASSOs = cv.glmnet(x = matrix.train, y = Y.train)
### Get both 'best' lambda values using $lambda.min and $lambda.1se
lambda.min = all.LASSOs$lambda.min
lambda.1se = all.LASSOs$lambda.1se
### cv.glmnet() has a predict() function (yay!). This predict function
### also does other things, like get the coefficients, or tell us
### which predictors get non-zero coefficients. We are also able
### to specify the value of lambda for which we want our output
### (remember that, with ridge, we got a matrix of coefficients and
### had to choose the row matching our lambda). Strangely, the name
### of the input where we specify our value of lambda is s.
### Get the coefficients for our two 'best' LASSO models
coef.LASSO.min = predict(all.LASSOs, s = lambda.min, type = "coef")
coef.LASSO.1se = predict(all.LASSOs, s = lambda.1se, type = "coef")
### Get which predictors are included in our models (i.e. which
### predictors have non-zero coefficients)
included.LASSO.min = predict(all.LASSOs, s = lambda.min,
type = "nonzero")
included.LASSO.1se = predict(all.LASSOs, s = lambda.1se,
type = "nonzero")
### Get predictions from both models on the validation fold. First,
### we need to create a predictor matrix from the validation set.
### Remember to include the intercept in model.matrix(), then delete
### it in the next step.
matrix.valid.LASSO.raw = model.matrix(alcohol ~ ., data = data.valid)
matrix.valid.LASSO = matrix.valid.LASSO.raw[,-1]
pred.LASSO.min = predict(all.LASSOs, newx = matrix.valid.LASSO,
s = lambda.min, type = "response")
pred.LASSO.1se = predict(all.LASSOs, newx = matrix.valid.LASSO,
s = lambda.1se, type = "response")
### Calculate MSPEs and store them
MSPE.LASSO.min = get.MSPE(Y.valid, pred.LASSO.min)
all.MSPEs[i, "LASSO-Min"] = MSPE.LASSO.min
MSPE.LASSO.1se = get.MSPE(Y.valid, pred.LASSO.1se)
all.MSPEs[i, "LASSO-1se"] = MSPE.LASSO.1se
}
library(MASS)   # For ridge regression
library(glmnet) # For LASSO
### There were some functions we defined and used last week which
### made life easier. I've put them in a separate R script so we
### can give ourselves access to them without running all of last
### week's code.
source("Helper Functions.R")
### We are going to do CV, so we need to run set.seed()
set.seed(93425633)
### Read-in the data, and keep all predictors
source("Read Wine Data - All Vars.R")
### Let's define a function for constructing CV folds
get.folds = function(n, K) {
### Get the appropriate number of fold labels
n.fold = ceiling(n / K) # Number of observations per fold (rounded up)
fold.ids.raw = rep(1:K, times = n.fold) # Generate extra labels
fold.ids = fold.ids.raw[1:n] # Keep only the correct number of labels
### Shuffle the fold labels
folds.rand = fold.ids[sample.int(n)]
return(folds.rand)
}
### Number of folds
K = 10
n = nrow(data) # Sample size
folds = get.folds(n, K)
### Create a container for MSPEs. Let's include ordinary least-squares
### regression for reference
all.models = c("LS", "Ridge", "LASSO-Min", "LASSO-1se")
all.MSPEs = array(0, dim = c(K, length(all.models)))
colnames(all.MSPEs) = all.models
for(i in 1:K){
### Split data
data.train = data[folds != i,]
data.valid = data[folds == i,]
n.train = nrow(data.train)
### Get response vectors
Y.train = data.train$alcohol
Y.valid = data.valid$alcohol
###################################################################
### First, let's quickly do LS so we have a reference point for ###
### how well the other models do.                               ###
###################################################################
fit.ls = lm(alcohol ~ ., data = data.train)
pred.ls = predict(fit.ls, newdata = data.valid)
MSPE.ls = get.MSPE(Y.valid, pred.ls)
all.MSPEs[i, "LS"] = MSPE.ls
#######################################################################
### Next, let's do ridge regression. This model is fit using the    ###
### lm.ridge() function in the MASS package. We will need to make a ###
### list of candidate lambda values for the function to choose      ###
### from. Prediction also has some extra steps, but we'll discuss   ###
### that when we get there.                                         ###
#######################################################################
### Make a list of lambda values. The lm.ridge() function will
### then choose the best value from this list. Use the seq()
### function to create an equally-spaced list.
lambda.vals = seq(from = 0, to = 100, by = 0.05)
### Use the lm.ridge() function to fit a ridge regression model. The
### syntax is almost identical to the lm() function, we just need
### to set lambda equal to our list of candidate values.
fit.ridge = lm.ridge(alcohol ~ ., lambda = lambda.vals,
data = data.train)
### To get predictions, we need to evaluate the fitted regression
### equation directly (sadly, no predict() function to do this for us).
### You could do this using a for loop if you prefer, but there is
### a shortcut which uses matrix-vector multiplication. The syntax
### for this multiplication method is much shorter.
### Get best lambda value and its index
### Note: Best is chosen according to smallest GCV value. We can
###       get GCV from a ridge regression object using $GCV
ind.min.GCV = which.min(fit.ridge$GCV)
lambda.min = lambda.vals[ind.min.GCV]
### Get coefficients corresponding to best lambda value
### We can get the coefficients for every value of lambda using
### the coef() function on a ridge regression object
all.coefs.ridge = coef(fit.ridge)
coef.min = all.coefs.ridge[ind.min.GCV,]
### We will multiply the dataset by this coefficients vector, but
### we need to add a column to our dataset for the intercept and
### create indicators for our categorical predictors. A simple
### way to do this is using the model.matrix() function from last
### week.
matrix.valid.ridge = model.matrix(alcohol ~ ., data = data.valid)
### Now we can multiply the data by our coefficient vector. The
### syntax in R for matrix-vector multiplication is %*%. Note that,
### for this type of multiplication, order matters. That is,
### A %*% B != B %*% A. Make sure you do data %*% coefficients.
### For more information, see me in a Q&A session or, better still,
### take a course on linear algebra (it's really neat stuff)
pred.ridge = matrix.valid.ridge %*% coef.min
### Now we just need to calculate the MSPE and store it
MSPE.ridge = get.MSPE(Y.valid, pred.ridge)
all.MSPEs[i, "Ridge"] = MSPE.ridge
#######################################################################
### Now we can do the LASSO. This model is fit using the glmnet()   ###
### or cv.glmnet() functions in the glmnet package. LASSO also has  ###
### a tuning parameter, lambda, which we have to choose.            ###
### Fortunately, the cv.glmnet() function does CV internally, and   ###
### lets us automatically find the 'best' value of lambda.          ###
#######################################################################
### The cv.glmnet() function has different syntax from what we're
### used to. Here, we have to provide a matrix with all of our
### predictors, and a vector of our response. LASSO handles
### the intercept differently, so we want to make sure our data
### matrix does not include an intercept (then let cv.glmnet() add
### an intercept later). Unfortunately, the model.matrix() function
### gets confused if we ask it to construct indicators for our
### categorical predictors without also including an intercept.
### A simple way to fix this is to create the data matrix with an
### intercept, then delete the intercept.
matrix.train.raw = model.matrix(alcohol ~ ., data = data.train)
matrix.train = matrix.train.raw[,-1]
### The cv.glmnet() function creates a list of lambda values, then
### does CV internally to choose the 'best' one. 'Best' can refer to
### either the value of lambda which gives the smallest CV-MSPE
### (called the min rule), or the value of lambda which gives the
### simplest model that gives CV-MSPE close to the minimum (called
### the 1se rule). The cv.glmnet() function gets both of these
### lambda values.
all.LASSOs = cv.glmnet(x = matrix.train, y = Y.train)
### Get both 'best' lambda values using $lambda.min and $lambda.1se
lambda.min = all.LASSOs$lambda.min
lambda.1se = all.LASSOs$lambda.1se
### cv.glmnet() has a predict() function (yay!). This predict function
### also does other things, like get the coefficients, or tell us
### which predictors get non-zero coefficients. We are also able
### to specify the value of lambda for which we want our output
### (remember that, with ridge, we got a matrix of coefficients and
### had to choose the row matching our lambda). Strangely, the name
### of the input where we specify our value of lambda is s.
### Get the coefficients for our two 'best' LASSO models
coef.LASSO.min = predict(all.LASSOs, s = lambda.min, type = "coef")
coef.LASSO.1se = predict(all.LASSOs, s = lambda.1se, type = "coef")
### Get which predictors are included in our models (i.e. which
### predictors have non-zero coefficients)
included.LASSO.min = predict(all.LASSOs, s = lambda.min,
type = "nonzero")
included.LASSO.1se = predict(all.LASSOs, s = lambda.1se,
type = "nonzero")
### Get predictions from both models on the validation fold. First,
### we need to create a predictor matrix from the validation set.
### Remember to include the intercept in model.matrix(), then delete
### it in the next step.
matrix.valid.LASSO.raw = model.matrix(alcohol ~ ., data = data.valid)
matrix.valid.LASSO = matrix.valid.LASSO.raw[,-1]
pred.LASSO.min = predict(all.LASSOs, newx = matrix.valid.LASSO,
s = lambda.min, type = "response")
pred.LASSO.1se = predict(all.LASSOs, newx = matrix.valid.LASSO,
s = lambda.1se, type = "response")
### Calculate MSPEs and store them
MSPE.LASSO.min = get.MSPE(Y.valid, pred.LASSO.min)
all.MSPEs[i, "LASSO-Min"] = MSPE.LASSO.min
MSPE.LASSO.1se = get.MSPE(Y.valid, pred.LASSO.1se)
all.MSPEs[i, "LASSO-1se"] = MSPE.LASSO.1se
}
boxplot(all.MSPEs, main = paste0("CV MSPEs over ", K, " folds"))
### Calculate RMSPEs
all.RMSPEs = apply(all.MSPEs, 1, function(W){
best = min(W)
return(W / best)
})
all.RMSPEs = t(all.RMSPEs)
boxplot(all.RMSPEs, main = paste0("CV RMSPEs over ", K, " folds"))
### One model is much worse than the others. Let's zoom in on the
### good models.
boxplot(all.RMSPEs, ylim = c(1, 1.03),
main = paste0("CV RMSPEs over ", K,
" folds (enlarged to show texture)"),
)
library(pls)    # For partial least squares
source("Helper Functions.R")
set.seed(31295905)
source("Read Wine Data - All Vars.R")
### model.matrix(), then deleting the intercept immediately after.
data.matrix.raw = model.matrix(alcohol ~ ., data = data)
data.matrix = data.matrix.raw[,-1]
fit.PCA = prcomp(data.matrix, scale. = T)
print(fit.PCA)
vars = fit.PCA$sdev^2
plot(1:length(vars), vars, main = "Variability Explained",
xlab = "Principal Component", ylab = "Variance Explained")
abline(h = 1)
c.vars = cumsum(vars)   ### Cumulative variance explained
rel.c.vars = c.vars / max(c.vars)   ### Cumulative proportion of
### variance explained
plot(1:length(rel.c.vars), rel.c.vars,
main = "Proportion of Variance Explained by First W PCs",
xlab = "W", ylab = "Proportion of Variance Explained")
c.vars = cumsum(vars)   ### Cumulative variance explained
rel.c.vars = c.vars / max(c.vars)   ### Cumulative proportion of
### variance explained
plot(1:length(rel.c.vars), rel.c.vars,
main = "Proportion of Variance Explained by First W PCs",
xlab = "W", ylab = "Proportion of Variance Explained")
all.PCs = fit.PCA$rotation
print(all.PCs[,1:5])
### Let's define a function for constructing CV folds
get.folds = function(n, K) {
### Get the appropriate number of fold labels
n.fold = ceiling(n / K) # Number of observations per fold (rounded up)
fold.ids.raw = rep(1:K, times = n.fold) # Generate extra labels
fold.ids = fold.ids.raw[1:n] # Keep only the correct number of labels
### Shuffle the fold labels
folds.rand = fold.ids[sample.int(n)]
return(folds.rand)
}
n = nrow(data) # Sample size
folds = get.folds(n, K)
### Create a container for MSPEs. Let's include ordinary least-squares
### regression for reference
all.models = c("LS", "PLS")
all.MSPEs = array(0, dim = c(K, length(all.models)))
colnames(all.MSPEs) = all.models
K = 10
### Construct folds
n = nrow(data) # Sample size
folds = get.folds(n, K)
### Create a container for MSPEs. Let's include ordinary least-squares
### regression for reference
all.models = c("LS", "PLS")
all.MSPEs = array(0, dim = c(K, length(all.models)))
colnames(all.MSPEs) = all.models
###################################################################
### First, let's quickly do LS so we have a reference point for ###
### how well the other models do.                               ###
###################################################################
fit.ls = lm(alcohol ~ ., data = data.train)
pred.ls = predict(fit.ls, newdata = data.valid)
MSPE.ls = get.MSPE(Y.valid, pred.ls)
all.MSPEs[i, "LS"] = MSPE.ls
### Begin cross-validation
for(i in 1:K){
### Split data
data.train = data[folds != i,]
data.valid = data[folds == i,]
n.train = nrow(data.train)
### Get response vector
Y.valid = data.valid$alcohol
### Now, let's do PLS using the plsr() function. The syntax is
### very similar to lm(). If we set validation = "CV", the plsr()
### function will do its own internal CV, and give MSPEs for each
### number of components. We can then use this to choose how many
### componenets to keep when doing prediction on the validation
### fold. We can use an optional input called segments to specify
### how many folds we want plsr() to use for its internal CV
### (default is 10).
fit.pls = plsr(alcohol ~ ., data = data.train, validation = "CV",
segments = 5)
### Investigate the fitted PLS model. Comment out the next two
### lines when running a CV loop
### The summary function gives us lots of information about how
### errors change as we increase the number of components
# summary(fit.pls)
### The validationplot() function shows how MSPE from the internal
### CV of plsr() changes with the number of included components.
# validationplot(fit.pls)
### Get the best model from PLS. To do this, we need to find the model
### that minimizes MSPE for the plsr() function's internal CV. It
### takes a few steps, but all the information we need is contained
### in the output of plsr().
CV.pls = fit.pls$validation # All the CV information
PRESS.pls = CV.pls$PRESS    # Sum of squared CV residuals
CV.MSPE.pls = PRESS.pls / nrow(data.train)  # MSPE for internal CV
ind.best.pls = which.min(CV.MSPE.pls) # Optimal number of components
### Get predictions and calculate MSPE on the validation fold
### Set ncomps equal to the optimal number of components
pred.pls = predict(fit.pls, data.valid, ncomp = ind.best.pls)
MSPE.pls = get.MSPE(Y.valid, pred.pls)
all.MSPEs[i, "PLS"] = MSPE.pls
}
### Make boxplots
boxplot(all.MSPEs, main = "MSPE for 10-fold CV")
library(stringr)
library(glmnet)
library(MASS)
set.seed(3014312)
n = 50 #  PLAY WITH THIS NUMBER
p = 10 #  PLAY WITH THIS NUMBER
beta1 = 1
sigma = 3 # PLAY WITH THIS NUMBER
iter = 100
coefs.lm = matrix(NA, nrow=iter, ncol=p+1)
coefs.sw = matrix(NA, nrow=iter, ncol=p+1)
coefs.ri = matrix(NA, nrow=iter, ncol=p+1)
coefs.la = matrix(NA, nrow=iter, ncol=p+1)
MSPEs = matrix(NA, nrow=iter, ncol=4)
colnames(MSPEs) = c("LM", "STEP", "RIDGE", "LASSO")
testx1 = rnorm(n=1000)
testx = cbind(testx1, matrix(0, ncol=(p-1), nrow=length(testx1)))
testx.all = cbind(testx1, matrix(rnorm(n=(p-1)*length(testx1)), ncol=(p-1), nrow=length(testx1)))
noms = c("X1")
for(q in 2:p){
noms = c(noms,paste0("X",q))
}
colnames(testx) = noms
colnames(testx.all) = colnames(testx)
testy = testx1*beta1
#Example Data Set
x = matrix(rnorm(n=n*p), nrow=n)
eps = rnorm(n, 0, sigma)
y = beta1*x[,1] + eps
x11()
curve(expr=beta1*x, from=-3, to=3,
ylim=c(-8,8),
col="red", lwd=3, xlab="X", ylab="Y",
main=paste("One data set with n=",n, "\n Population R-squared=",
round(1/(1+sigma^2),2)))
points(x=x[,1], y=y, pch="X", col="blue")
for(i in 1:iter){
x = matrix(rnorm(n=n*p), nrow=n)
eps = rnorm(n, 0, sigma)
y = beta1*x[,1] + eps
xydat = data.frame(y,x)
mod.lm = lm(y~., data=xydat)
coefs.lm[i,] = coef(mod.lm)
step1 <- step(object=lm(y~1, data=xydat), scope=list(upper=mod.lm), direction="both",
k = log(nrow(xydat)), trace=0)
coef.locs = c(1, 1+as.numeric(str_remove_all(names(coef(step1))[-1], "X")))
coefs.sw[i,] = 0
coefs.sw[i,coef.locs] = coef(step1)
ridgec <- lm.ridge(y ~., lambda = seq(0, 100, .05), data=xydat)
coefs.ri[i,] = coef(ridgec)[which.min(ridgec$GCV),]
cv.lasso.1 <- cv.glmnet(y=y, x= x, family="gaussian")
coefs.la[i,] = coef(cv.lasso.1)[,1]
pred.lm = predict(mod.lm, as.data.frame(testx.all))
pred.sw = predict(step1, as.data.frame(testx.all))
pred.ri = as.matrix(cbind(1,testx.all)) %*% coef(ridgec)[which.min(ridgec$GCV),]
pred.la = predict(cv.lasso.1, testx.all, s="lambda.min")
MSPEs[i,] = c(mean((testy-pred.lm)^2),
mean((testy-pred.sw)^2),
mean((testy-pred.ri)^2),
mean((testy-pred.la)^2))
}
boxplot(MSPEs[,1:2],
main=paste0("Comparison of MSPEs\n R-squared=",
round(1/(1+sigma^2),2), ", n=",n,", p=",p),
names=c("lm","step"))
x11()
boxplot(MSPEs,
main=paste0("Comparison of MSPEs\n R-squared=",
round(1/(1+sigma^2),2), ", n=",n,", p=",p),
names=c("lm","step", "ridge","LASSO"))
MSE = matrix(NA, nrow=p+1, ncol=4)
for(j in 2:3){
truec = ifelse(j-1==1, 1, 0)
x11(height=7, width=7)
boxplot(cbind(coefs.lm[,j], coefs.sw[,j], coefs.ri[,j], coefs.la[,j]),
main=paste0("Comparison of coefs for variable ",j-1,
"\n R-squared=",round(1/(1+sigma^2),2), ", n=",n,", p=",p),
names=c("lm","step","Ridge", "LASSO"))
abline(h=truec, col="red")
points(x=1, y=mean(coefs.lm[,j]), col="blue")
points(x=2, y=mean(coefs.sw[,j]), col="blue")
points(x=3, y=mean(coefs.ri[,j]), col="blue")
points(x=4, y=mean(coefs.la[,j]), col="blue")
#  x11(height=7, width=7)
#  boxplot(cbind(coefs.lm[,j], coefs.sw[,j], coefs.ri[,j]),
#          main=paste0("Comparison of coefs for variable ",j-1,
#                      "\n R-squared=",round(1/(1+sigma^2),2), ", n=",n,", p=",p),
#          names=c("lm","step","Ridge"))
#  abline(h=truec, col="red")
#  points(x=1, y=mean(coefs.lm[,j]), col="blue")
#  points(x=2, y=mean(coefs.sw[,j]), col="blue")
#  points(x=3, y=mean(coefs.ri[,j]), col="blue")
MSE[j,]=round(c(mean((coefs.lm[,j]-truec)^2),
mean((coefs.sw[,j]-truec)^2),
mean((coefs.ri[,j]-truec)^2),
mean((coefs.la[,j]-truec)^2)), 3)
#  x11(height=7, width=7)
#  boxplot(cbind(coefs.lm[,j], coefs.sw[,j]),
#          main=paste0("Comparison of coefs for variable ",j-1,
#                      "\n R-squared=",round(1/(1+sigma^2),2), ", n=",n,", p=",p),
#          names=c("lm","step"))
#  abline(h=truec, col="red")
#  points(x=1, y=mean(coefs.lm[,j]), col="blue")
#  points(x=2, y=mean(coefs.sw[,j]), col="blue")
#  x11(h=5, w=10)
#  par(mfrow = c(1,2))
#  hist(coefs.lm[,j], breaks = seq(-2.25, 2.25, 0.5)+truec, ylim=c(0,100),
#       main=paste0("Histogram of beta-hat", j, " from LS"))
#  abline(v=truec, col="red")
#  abline(v=mean(coefs.lm[,j]), col="blue")
#  hist(coefs.sw[,j], breaks = seq(-2.25, 2.25, 0.5)+truec, ylim=c(0,100),
#       main=paste0("Histogram of beta-hat", j, " from Step"))
#  abline(v=truec, col="red")
#  abline(v=mean(coefs.sw[,j]), col="blue")
#  x11(h=5, w=10)
#  par(mfrow = c(1,2))
#  hist(coefs.ri[,j], breaks = seq(-2.25, 2.25, 0.5)+truec, ylim=c(0,100),
#       main=paste0("Histogram of beta-hat", j, " from Ridge"))
#  abline(v=truec, col="red")
#  abline(v=mean(coefs.ri[,j]), col="blue")
#  hist(coefs.la[,j], breaks = seq(-2.25, 2.25, 0.5)+truec, ylim=c(0,100),
#       main=paste0("Histogram of beta-hat", j, " from LASSO"))
#  abline(v=truec, col="red")
#  abline(v=mean(coefs.la[,j]), col="blue")
}
MSE
nonz = function(x){mean(x!=0)}
round(rbind(
apply(X=coefs.lm, MARGIN=2, FUN=nonz),
apply(X=coefs.sw, MARGIN=2, FUN=nonz),
apply(X=coefs.la, MARGIN=2, FUN=nonz)), 2)
