ylim=c(min(y)-.2, max(y)+.2),  #c(0.5,3.8),
col="red", lwd=1, xlab="X", ylab="Y", lty="dotted" ,
main=paste("Estimated regression line vs. truth"))
#####
points(x=x, y=y, pch="x", col="blue")
######
# Estimating the regression line
mod = lm(y~x+I(x^2)+I(x^3)+I(x^4))
#abline(mod1, col="brown", lwd=2)
predfun=function(x){predict(mod,data.frame(x))}
curve(expr=predfun, col="blue", lwd=3, add=TRUE)
####################################
# Evaluate the regression process
# Repeat for 100 samples
curve(expr=beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, from=-1, to=1,
ylim=c(min(y)-6*sigma, max(y)+6*sigma),  #c(0.5,3.8),
col="red", lwd=3, xlab="X", ylab="Y",
main=paste("Comparing many regression curves \nfrom 1000 different samples of n=",n))
#####
betas = matrix(NA, nrow=100, ncol=5)
for(jj in 1:100){
x = runif(n=n, min=-1, max=1)
epsilon = rnorm(n=n, mean=0, sd=sigma)
y =  beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + epsilon
mod = lm(y~x+I(x^2)+I(x^3)+I(x^4))
betas[jj,] = coef(mod)
curve(expr=predfun, col="lightblue", add=TRUE)
}
curve(expr=beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, from=-1, to=1,
col="red", lwd=2, add=TRUE)
###############################
# Focus on the comparison between true mean and average regression
avbet = apply(betas, 2, mean)
curve(expr=avbet[1] + avbet[2]*x + avbet[3]*x^2 + avbet[4]*x^3 + avbet[5]*x^4, from=-1, to=1,
col="blue", lwd=2, xlab="X", ylab="Y",
main=paste("How well does regression estimate \ntrue mean on average?"))
curve(expr=beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, from=-1, to=1,
col="red", lwd=2, add=TRUE)
#avbet = apply(betas, 2, mean)
#curve(expr=avbet[1] + avbet[2]*x + avbet[3]*x^2 + avbet[4]*x^3 + avbet[5]*x^4, from=-1, to=1,
#      col="blue", lwd=2, xlab="X", ylab="Y",
#      main=paste("Bias might be reduced if we fit a better model"))
#curve(expr=beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, from=-1, to=1,
#      col="red", lwd=2, add=TRUE)
library(TSA)
data(gold)
gold
#(b)Plot the dataset. Remember to label the x- and y-axes. Briefly describe what you see.
plot(gold, ylab="price of gold", xlab="Time", type="o")
model = lm(gold ~ time(gold))
summary(model)
win.graph(width = 4.875, height=2.5, pointsize=8)
plot(gold, type="o", ylab="price of gold", xlab = "Time")
abline(model)
abline(model)
model = lm(gold ~ time(gold))
summary(model)
# (f) Plot the (studentized) residuals of the model against time. Describe what you see in the plot.
plot(y=restudent(model), x=as.vector(time(gold)), xlab = "Time", ylab="Standardized Residuals", type="l")
# (f) Plot the (studentized) residuals of the model against time. Describe what you see in the plot.
plot(y=rstudent(model), x=as.vector(time(gold)), xlab = "Time", ylab="Standardized Residuals", type="l")
plot(gold, ylab="price of gold", xlab="Time", type="o")
quadratic_model = lm (gold ~ time(gold + I(time(gold)^2)))
win.graph(width = 4.875, height=2.5, pointsize=8)
plot(gold, type="o", ylab="price of gold", xlab = "Time")
abline(quadratic_model)
quadratic_model = lm (gold ~ time(gold + I(time(gold)^2)))
win.graph(width = 4.875, height=2.5, pointsize=8)
plot(gold, type="o", ylab="price of gold", xlab = "Time")
abline(quadratic_model)
quadratic_model = lm (gold ~ time(gold + I(time(gold)^2)))
quadratic_model = lm (gold ~ time(gold + I(time(gold)^2)))
summary(quadratic_model)
quadratic_model = lm (gold ~ time(gold) + I(time(gold)^2))
summary(quadratic_model)
win.graph(width = 4.875, height=2.5, pointsize=8)
plot(gold, type="o", ylab="price of gold", xlab = "Time")
abline(quadratic_model)
quadratic_model = lm(gold ~ time(gold) + I(time(gold)^2))
summary(quadratic_model)
win.graph(width = 4.875, height=2.5, pointsize=8)
plot(gold, type="o", ylab="price of gold", xlab = "Time")
abline(quadratic_model)
quadratic_model = lm(gold ~ time(gold) + I(time(gold)^2))
summary(quadratic_model)
quadratic_model = lm(gold ~ time(gold) + I(time(gold)^2))
summary(quadratic_model)
win.graph(width = 4.875, height=2.5, pointsize=8)
plot(gold, type="o", ylab="price of gold", xlab = "Time")
abline(quadratic_model)
plot(gold, type="o", ylab="price of gold", xlab = "Time")
abline(quadratic_model)
#(h) Plot the (studentized) residuals of the quadratic model against time. What do you see now?
plot(y=rstudent(quadratic_model), x=as.vector(time(gold)), xlab = "Time", ylab="Standardized Residuals", type="l")
data(retail)
month. = season(retail)
month.retail = lm(retail ~ month.-1 + time(retail))
quadratic_model = lm(gold ~ time(gold) + I(time(gold)^2))
summary(quadratic_model)
library(dplyr) # I like using filter and select sometimes
library(leaps) # This package contains the regsubsets() function,
# which does all subsets selection
set.seed(6588719)
#####################################################################
### If you find yourself writing code to do the same thing        ###
### repeatedly, it is often helpful to write a function that does ###
### that thing. Let's make some such functions here.              ###
#####################################################################
### We will regularly need to shuffle a vector. This function
### does that for us.
shuffle = function(X){
new.order = sample.int(length(X))
new.X = X[new.order]
return(new.X)
}
### We will also often need to calculate MSE using an observed
### and a prediction vector. This will be another useful function.
get.MSPE = function(Y, Y.hat){
return(mean((Y - Y.hat)^2))
}
### The way that the lm() function calculates predicted values
### is somewhat limited. Specifically, the predict function only
### works if our new data are in a data frame which contains
### columns who's names match the variables in our model. Sometimes,
### we will want to format the new data as a matrix. This function
### lets us still get predicted values.
### Note: This function uses matrix-vector multiplication via the
### %*% operation. If you have taken a course in linear algebra, you
### will have seen how useful this tool can be. If not, don't
### worry, you don't need to understand the details of this function
predict.matrix = function(fit.lm, X.mat){
coeffs = fit.lm$coefficients
Y.hat = X.mat %*% coeffs
return(Y.hat)
}
source("Read Wine Data - Categorical.R")
head(data)
library(dplyr)
library(leaps) # This package contains the regsubsets() function,
data = na.omit(airquality[, 1:4])
data$TWcp = data$Temp*AQ$Wind
data$TWrat = data$Temp/AQ$Wind
data = na.omit(airquality[, 1:4])
data$TWcp = data$Temp* data$Wind
data$TWrat = data$Temp/data$Wind
n = nrow(data)
n.train = floor(n * 0.75)
n.valid = n - n.train
groups = c(rep(1, times = n.train), rep(2, times = n.valid))
groups.shuffle = shuffle(groups)
data.train = data[groups.shuffle == 1,]
data.valid = data[groups.shuffle == 2,]
shuffle = function(X){
new.order = sample.int(length(X))
new.X = X[new.order]
return(new.X)
}
get.MSPE = function(Y, Y.hat){
return(mean((Y - Y.hat)^2))
}
########################################################################
### Create a container to store MSPEs
all.models = c("Solar", "Wind", "Temp", "TWcp", "TWrat")
all.MSPEs = rep(0, times = length(all.models))
names(all.MSPEs) = all.models
#############################################
### All subsets selection via AIC and BIC ###
#############################################
### The model.matrix lets us specify the regression formula we want
### to use, and outputs the corresponding model matrix
data.matrix = model.matrix(Ozone ~ .^2, data = data.train)
### We also need the response variable (i.e. alcohol)
Y.train = data.train$Ozone
### Now we can run all.subsets. There are a couple of extra inputs
### here. nvmax is the largest number of variables we are willing
### to include. 30 seems like plenty. intercept specifies whether we
### want regsubsets() to add an intercept term. Since model.matrix()
### already adds an intercept, we don't want regsubsets() to add
### another one.
all.subsets = regsubsets(x = data.matrix, y = Y.train, nvmax = 30,
intercept = F)
library(dplyr)
library(leaps) # This package contains the regsubsets() function,
# which does all subsets selection
data = na.omit(airquality[, 1:4])
data$TWcp = data$Temp* data$Wind
data$TWrat = data$Temp/data$Wind
n = nrow(data)
n
n = nrow(data)
n.train = floor(n * 0.75)
n.valid = n - n.train
groups = c(rep(1, times = n.train), rep(2, times = n.valid))
groups.shuffle = shuffle(groups)
data.train = data[groups.shuffle == 1,]
data.valid = data[groups.shuffle == 2,]
library(dplyr)
library(leaps) # This package contains the regsubsets() function,
# which does all subsets selection
shuffle = function(X){
new.order = sample.int(length(X))
new.X = X[new.order]
return(new.X)
}
get.MSPE = function(Y, Y.hat){
return(mean((Y - Y.hat)^2))
}
data = na.omit(airquality[, 1:4])
data$TWcp = data$Temp* data$Wind
data$TWrat = data$Temp/data$Wind
n = nrow(data)
n.train = floor(n * 0.75)
n.valid = n - n.train
groups = c(rep(1, times = n.train), rep(2, times = n.valid))
groups.shuffle = shuffle(groups)
data.train = data[groups.shuffle == 1,]
data.valid = data[groups.shuffle == 2,]
all.models = c("Solar", "Wind", "Temp", "TWcp", "TWrat")
all.MSPEs = rep(0, times = length(all.models))
names(all.MSPEs) = all.models
data.matrix = model.matrix(Ozone ~ .^2, data = data.train)
### We also need the response variable (i.e. alcohol)
Y.train = data.train$Ozone
### Now we can run all.subsets. There are a couple of extra inputs
### here. nvmax is the largest number of variables we are willing
### to include. 30 seems like plenty. intercept specifies whether we
### want regsubsets() to add an intercept term. Since model.matrix()
### already adds an intercept, we don't want regsubsets() to add
### another one.
all.subsets = regsubsets(x = data.matrix, y = Y.train, nvmax = 30,
intercept = F)
all.subsets = regsubsets(x = data.matrix, y = Y.train, nvmax = 30,
intercept = F)
data.matrix = model.matrix(Ozone ~ .^2, data = data.train)
Y.train = data.train$Ozone
library(dplyr) # I like using filter and select sometimes
library(leaps) # This package contains the regsubsets() function,
# which does all subsets selection
set.seed(6588719)
#####################################################################
### If you find yourself writing code to do the same thing        ###
### repeatedly, it is often helpful to write a function that does ###
### that thing. Let's make some such functions here.              ###
#####################################################################
### We will regularly need to shuffle a vector. This function
### does that for us.
shuffle = function(X){
new.order = sample.int(length(X))
new.X = X[new.order]
return(new.X)
}
### We will also often need to calculate MSE using an observed
### and a prediction vector. This will be another useful function.
get.MSPE = function(Y, Y.hat){
return(mean((Y - Y.hat)^2))
}
### The way that the lm() function calculates predicted values
### is somewhat limited. Specifically, the predict function only
### works if our new data are in a data frame which contains
### columns who's names match the variables in our model. Sometimes,
### we will want to format the new data as a matrix. This function
### lets us still get predicted values.
### Note: This function uses matrix-vector multiplication via the
### %*% operation. If you have taken a course in linear algebra, you
### will have seen how useful this tool can be. If not, don't
### worry, you don't need to understand the details of this function
predict.matrix = function(fit.lm, X.mat){
coeffs = fit.lm$coefficients
Y.hat = X.mat %*% coeffs
return(Y.hat)
}
source("Read Wine Data - Categorical.R")
setwd("C:/Users/injoo/OneDrive/Desktop/SFU/STAT 452/Tutorial/Lecture4_5")
### Use a new script to read-in and clean the dataset. The new version
### keeps quality and type. It also recodes quality into low,
### medium and high
source("Read Wine Data - Categorical.R")
head(data)
n = nrow(data)
n.train = floor(n * 0.75)
n.valid = n - n.train
groups = c(rep(1, times = n.train), rep(2, times = n.valid))
groups.shuffle = shuffle(groups)
data.train = data[groups.shuffle == 1,]
data.valid = data[groups.shuffle == 2,]
all.models = c("subsets.AIC", "subsets.BIC",
"stepwise.AIC", "stepwise.BIC")
all.MSPEs = rep(0, times = length(all.models))
names(all.MSPEs) = all.models
all.MSPEs
data.matrix = model.matrix(alcohol ~ .^2, data = data.train)
data.matrix
Y.train = data.train$alcohol
all.subsets = regsubsets(x = data.matrix, y = Y.train, nvmax = 30,
intercept = F)
all.subsets
info.subsets = summary(all.subsets)
all.subsets.models = info.subsets$which
all.subsets.models
library(dplyr)
library(leaps) # This package contains the regsubsets() function,
# which does all subsets selection
shuffle = function(X){
new.order = sample.int(length(X))
new.X = X[new.order]
return(new.X)
}
get.MSPE = function(Y, Y.hat){
return(mean((Y - Y.hat)^2))
}
data = na.omit(airquality[, 1:4])
data$TWcp = data$Temp* data$Wind
data$TWrat = data$Temp/data$Wind
n = nrow(data)
n.train = floor(n * 0.75)
n.valid = n - n.train
groups = c(rep(1, times = n.train), rep(2, times = n.valid))
groups.shuffle = shuffle(groups)
data.train = data[groups.shuffle == 1,]
data.valid = data[groups.shuffle == 2,]
#1. Use all-subsets regression.
# (a) Report the variables in the best model of each size.
########################################################################
### Create a container to store MSPEs
all.models = c("Solar", "Wind", "Temp", "TWcp", "TWrat")
all.MSPEs = rep(0, times = length(all.models))
names(all.MSPEs) = all.models
#############################################
### All subsets selection via AIC and BIC ###
#############################################
### The model.matrix lets us specify the regression formula we want
### to use, and outputs the corresponding model matrix
data.matrix = model.matrix(Ozone ~ .^2, data = data.train)
Y.train = data.train$Ozone
all.subsets = regsubsets(x = data.matrix, y = Y.train, nvmax = 10,
intercept = F)
library(dplyr) # I like using filter and select sometimes
library(leaps) # This package contains the regsubsets() function,
# which does all subsets selection
set.seed(6588719)
#####################################################################
### If you find yourself writing code to do the same thing        ###
### repeatedly, it is often helpful to write a function that does ###
### that thing. Let's make some such functions here.              ###
#####################################################################
### We will regularly need to shuffle a vector. This function
### does that for us.
shuffle = function(X){
new.order = sample.int(length(X))
new.X = X[new.order]
return(new.X)
}
### We will also often need to calculate MSE using an observed
### and a prediction vector. This will be another useful function.
get.MSPE = function(Y, Y.hat){
return(mean((Y - Y.hat)^2))
}
### The way that the lm() function calculates predicted values
### is somewhat limited. Specifically, the predict function only
### works if our new data are in a data frame which contains
### columns who's names match the variables in our model. Sometimes,
### we will want to format the new data as a matrix. This function
### lets us still get predicted values.
### Note: This function uses matrix-vector multiplication via the
### %*% operation. If you have taken a course in linear algebra, you
### will have seen how useful this tool can be. If not, don't
### worry, you don't need to understand the details of this function
predict.matrix = function(fit.lm, X.mat){
coeffs = fit.lm$coefficients
Y.hat = X.mat %*% coeffs
return(Y.hat)
}
##############################
### Now on to our analysis ###
##############################
### Use a new script to read-in and clean the dataset. The new version
### keeps quality and type. It also recodes quality into low,
### medium and high
source("Read Wine Data - Categorical.R")
head(data)
n = nrow(data)
n.train = floor(n * 0.75)
n.valid = n - n.train
groups = c(rep(1, times = n.train), rep(2, times = n.valid))
groups.shuffle = shuffle(groups)
data.train = data[groups.shuffle == 1,]
data.valid = data[groups.shuffle == 2,]
### Note: We split the data into training and validation sets before
### doing any kind of analysis. This is because we want to avoid
### letting the validation set influence any modelling decisions
### We should check what these new variables look like. We can get counts
### using the table() function
table(data.train$type)
table(data.train$quality)
data.matrix = model.matrix(alcohol ~ .^2, data = data.train)
### We also need the response variable (i.e. alcohol)
Y.train = data.train$alcohol
### Now we can run all.subsets. There are a couple of extra inputs
### here. nvmax is the largest number of variables we are willing
### to include. 30 seems like plenty. intercept specifies whether we
### want regsubsets() to add an intercept term. Since model.matrix()
### already adds an intercept, we don't want regsubsets() to add
### another one.
all.subsets = regsubsets(x = data.matrix, y = Y.train, nvmax = 30,
intercept = F)
library(dplyr)
library(leaps) # This package contains the regsubsets() function,
# which does all subsets selection
shuffle = function(X){
new.order = sample.int(length(X))
new.X = X[new.order]
return(new.X)
}
get.MSPE = function(Y, Y.hat){
return(mean((Y - Y.hat)^2))
}
data = na.omit(airquality[, 1:4])
data
data$TWcp = data$Temp* data$Wind
data$TWrat = data$Temp/data$Wind
head(data)
n = nrow(data)
n.train = floor(n * 0.75)
n.valid = n - n.train
groups = c(rep(1, times = n.train), rep(2, times = n.valid))
groups.shuffle = shuffle(groups)
data.train = data[groups.shuffle == 1,]
data.valid = data[groups.shuffle == 2,]
data.train
data.matrix = model.matrix(Ozone ~ .^2, data = data.train)
Y.train = data.train$Ozone
all.subsets = regsubsets(x = data.matrix, y = Y.train, nvmax = 10,
intercept = F)
all.subsets = regsubsets(x = data.matrix, y = Y.train, nvmax = 10,
intercept = F)
all.subsets = regsubsets(x = data.matrix, y = Y.train, nvmax = 10,
intercept = F)
all.subsets = regsubsets(x = data.matrix, y = Y.train, nvmax = 10,
intercept = F)
all.subsets = regsubsets(x = data.matrix, y = Y.train, nvmax = 10,
intercept = F)
all.subsets = regsubsets(x = data.matrix, y = Y.train, nvmax = 10,
intercept = F)
all.subsets = regsubsets(x = data.matrix, y = Y.train, nvmax = 10,
intercept = F)
ndim(data.matrix)
dim(data.matrix)
library(dplyr)
library(leaps) # This package contains the regsubsets() function,
# which does all subsets selection
shuffle = function(X){
new.order = sample.int(length(X))
new.X = X[new.order]
return(new.X)
}
get.MSPE = function(Y, Y.hat){
return(mean((Y - Y.hat)^2))
}
data = na.omit(airquality[, 1:4])
n = nrow(data)
n.train = floor(n * 0.75)
n.valid = n - n.train
groups = c(rep(1, times = n.train), rep(2, times = n.valid))
groups.shuffle = shuffle(groups)
data.train = data[groups.shuffle == 1,]
data.valid = data[groups.shuffle == 2,]
data.matrix = model.matrix(Ozone ~ .^2, data = data.train)
### We also need the response variable (i.e. alcohol)
Y.train = data.train$Ozone
### Now we can run all.subsets. There are a couple of extra inputs
### here. nvmax is the largest number of variables we are willing
### to include. 30 seems like plenty. intercept specifies whether we
### want regsubsets() to add an intercept term. Since model.matrix()
### already adds an intercept, we don't want regsubsets() to add
### another one.
all.subsets = regsubsets(x = data.matrix, y = Y.train, nvmax = 10,
intercept = F)
library(dplyr)
library(leaps) # This package contains the regsubsets() function,
# which does all subsets selection
shuffle = function(X){
new.order = sample.int(length(X))
new.X = X[new.order]
return(new.X)
}
get.MSPE = function(Y, Y.hat){
return(mean((Y - Y.hat)^2))
}
data = na.omit(airquality[, 1:4])
data$TWcp = data$Temp* data$Wind
data$TWrat = data$Temp/data$Wind
n = nrow(data)
n.train = floor(n * 0.75)
n.valid = n - n.train
groups = c(rep(1, times = n.train), rep(2, times = n.valid))
groups.shuffle = shuffle(groups)
data.train = data[groups.shuffle == 1,]
data.valid = data[groups.shuffle == 2,]
data.matrix = model.matrix(Ozone ~ .^2, data = data.train)
### We also need the response variable (i.e. alcohol)
Y.train = data.train$Ozone
### Now we can run all.subsets. There are a couple of extra inputs
### here. nvmax is the largest number of variables we are willing
### to include. 30 seems like plenty. intercept specifies whether we
### want regsubsets() to add an intercept term. Since model.matrix()
### already adds an intercept, we don't want regsubsets() to add
### another one.
all.subsets = regsubsets(x = data.matrix, y = Y.train, nvmax = 10,
intercept = F)
data.matrix
all.subsets = regsubsets(x = data.matrix, y = Y.train, nvmax = 10,
intercept = F)
